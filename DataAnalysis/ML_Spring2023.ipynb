{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b102cd",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression with Sparse Features\n",
    "\n",
    "In many real-world scenarios our data has millions of dimensions, but a given example has only hundreds of non-zero features. For example, in document analysis with word counts for features, our dictionary may have millions of words, but a given document has only hundreds of unique words. In this question we will make $l_2$ regularized SGD efficient when our input data is sparse. Recall that in $l_2$ regularized logistic regression, we want to maximize the following objective (in this problem we have excluded $w_0$ for simplicity):\n",
    "\n",
    "$$F(\\mathbf{w})=\\frac{1}{N} \\sum_{j=1}^N l\\left(\\mathbf{x}^{(j)}, y^{(j)}, \\mathbf{w}\\right)-\\frac{\\lambda}{2} \\sum_{i=1}^d \\mathbf{w}_i^2$$\n",
    "\n",
    "where $l\\left(\\mathbf{x}^{(j)}, y^{(j)}, \\mathbf{w}\\right)$ is the logistic objective function\n",
    "\n",
    "$$l\\left(\\mathbf{x}^{(j)}, y^{(j)}, \\mathbf{w}\\right)=y^{(j)}\\left(\\sum_{i=1}^d \\mathbf{w}_i \\mathbf{x}_i^{(j)}\\right)-\\ln \\left(1+\\exp \\left(\\sum_{i=1}^d \\mathbf{w}_i \\mathbf{x}_i^{(j)}\\right)\\right)$$\n",
    "\n",
    "and the remaining sum is our regularization penalty. When we do stochastic gradient descent on point $\\left(\\mathbf{x}^{(j)}, y^{(j)}\\right)$, we are approximating the objective function as\n",
    "\n",
    "$$F(\\mathbf{w}) \\approx l\\left(\\mathbf{x}^{(j)}, y^{(j)}, \\mathbf{w}\\right)-\\frac{\\lambda}{2} \\sum_{i=1}^d \\mathbf{w}_i^2$$\n",
    "\n",
    "Definition of sparsity: Assume that our input data has $d$ features, i.e. $\\mathbf{x}^{(j)} \\in \\mathbb{R}^d$. In this problem, we will consider the scenario where $\\mathbf{x}^{(j)}$ is sparse. Formally, let $s$ be average number of nonzero elements in each example. We say the data is sparse when $s<<d$. In the following questions, your answer should take the sparsity of $\\mathbf{x}^{(j)}$ into consideration when possible. Note: When we use a sparse data structure, we can iterate over the non-zero elements in $O(s)$ time, whereas a dense data structure requires $O(d)$ time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f66de",
   "metadata": {},
   "source": [
    "a. [1 point] Let us first consider the case when $\\lambda=0$. Write down the SGD update rule for $\\mathbf{w}_i$ when $\\lambda=0$, using step size $\\eta$, given the example $\\left(\\mathbf{x}^{(j)}, y^{(j)}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98cff0d",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "When $\\lambda=0$, the regularization term is removed. The gradient of the logistic loss function $l$ with respect to $\\mathbf{w}_i$ is:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial l(\\mathbf{x}^{(j)}, y^{(j)}, \\mathbf{w})}{\\partial \\mathbf{w}_i} &= \\left( y^{(j)} \\mathbf{x}_i^{(j)} - \\frac{\\exp(\\sum_{i=1}^d \\mathbf{w}_i \\mathbf{x}_i^{(j)})}{1 + \\exp(\\sum_{i=1}^d \\mathbf{w}_i \\mathbf{x}_i^{(j)})} \\mathbf{x}_i^{(j)} \\right) \\\\\n",
    "&=\\left(y^{(j)} - \\frac{1}{1 + \\exp(-\\sum_{i=1}^d \\mathbf{w}_i \\mathbf{x}_i^{(j)})}\\right) \\mathbf{x}_i^{(j)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Thus, the SGD update rule for $\\mathbf{w}_i$ when $\\lambda=0$ using a step size $\\eta$ for the example $(x^{(j)}, y^{(j)})$ is:\n",
    "\n",
    "$$\\mathbf{w}_i \\leftarrow \\mathbf{w}_i + \\eta \\left(y^{(j)} - \\frac{1}{1 + \\exp(-\\sum_{i=1}^d \\mathbf{w}_i \\mathbf{x}_i^{(j)})}\\right) \\mathbf{x}_i^{(j)}$$\n",
    "\n",
    "Note that the sign before $\\eta$ is '+'\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0a170c",
   "metadata": {},
   "source": [
    "b. [2 points] If we use a dense data structure, what is the average time complexity to update $\\mathbf{w}_i$ when $\\lambda=0$ ? What if we use a sparse data structure? Justify your answer in one or two sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b930b52",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "  \n",
    "- **Dense Data Structure**: The time complexity is $O(d)$ as we need to iterate through all $d$ features for each update to compute the sum $\\sum_{i=1}^d \\mathbf{w}_i \\mathbf{x}_i^{(j)}$ regardless of whether it is zero or not.\n",
    "- **Sparse Data Structure**: The time complexity reduces to $O(s)$, where $s$ is the average number of non-zero elements in each example, because we only iterate over the non-zero elements.\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6219b3d",
   "metadata": {},
   "source": [
    "c. [1 point] Now let us consider the general case when $\\lambda>0$. Write down the SGD update rule for $\\mathbf{w}_i$ when $\\lambda>0$, using step size $\\eta$, given the example $\\left(\\mathbf{x}^{(j)}, y^{(j)}\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa4670",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "When $\\lambda>0$, we need to include the regularization term in the gradient calculation. The regularized gradient becomes:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}_i} \\left( l(\\mathbf{x}^{(j)}, y^{(j)}, \\mathbf{w}) - \\frac{\\lambda}{2} \\mathbf{w}_i^2 \\right)\n",
    "$$\n",
    "\n",
    "The gradient of the regularization term is \n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mathbf{w}_i} (- \\frac{\\lambda}{2}\\sum_{i=1}^{d} \\mathbf{w}_i^2) = - \\lambda \\mathbf{w}_i\n",
    "$$\n",
    "\n",
    "\n",
    "The update rule becomes:\n",
    "$$\n",
    "\\mathbf{w}_i \\leftarrow \\mathbf{w}_i + \\eta \\left( y^{(j)} \\mathbf{x}_i^{(j)} - \\frac{1}{1 + \\exp(-\\sum_{i=1}^d \\mathbf{w}_i \\mathbf{x}_i^{(j)})} \\mathbf{x}_i^{(j)} - \\lambda \\mathbf{w}_i \\right)\n",
    "$$\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9534b6ad",
   "metadata": {},
   "source": [
    "d. [1 point] If we use a dense data structure, what is the average time complexity to update $\\mathbf{w}_i$ when $\\lambda>0$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a96275",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "With a dense data structure, the time complexity remains $O(d)$ for each update, similar to the $\\lambda=0$ case, as we still need to iterate through all features to compute the gradient of the logistic objective function.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a6db53",
   "metadata": {},
   "source": [
    "e. $\\left[2\\right.$ points] Let $\\mathbf{w}_i^{(t)}$ be the weight vector after $t$-th update. Now imagine that we perform $k$ SGD updates on $\\mathrm{w}$ using examples $\\left(\\mathbf{x}^{(t+1)}, y^{(t+1)}\\right), \\cdots,\\left(\\mathbf{x}^{(t+k)}, y^{(t+k)}\\right)$, where $\\mathbf{x}_i^{(j)}=0$ for every example in the sequence. (i.e. the $i$-th feature is zero for all of the examples in the sequence). Express the new weight, $\\mathbf{w}_i^{(t+k)}$ in terms of $\\mathbf{w}_i^{(t)}, k, \\eta$, and $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648aa8a3",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "Given that $\\mathbf{x}_i^{(j)}=0$ for every example in the sequence, the update rule simplifies to only include the regularization term. Therefore, after $k$ updates, the weight $\\mathbf{w}_i$ is updated as follows:\n",
    "\n",
    "$\\mathbf{w}_i^{(t+1)} = \\mathbf{w}_i^{(t)} - \\eta \\lambda \\mathbf{w}_i^{(t)} = (1 - \\eta \\lambda)\\mathbf{w}_i^{(t)}$\n",
    "\n",
    "$\\mathbf{w}_i^{(t+2)} = \\mathbf{w}_i^{(t+1)} - \\eta \\lambda \\mathbf{w}_i^{(t+1)} \\\\\n",
    "= (1 - \\eta \\lambda) \\mathbf{w}_i^{(t+1)} \\\\\n",
    "= (1 - \\eta \\lambda)^2\\mathbf{w}_i^{(t)}$\n",
    "\n",
    "\n",
    "Applying this iteratively for $k$ steps:\n",
    "\n",
    "$\n",
    "\\mathbf{w}_i^{(t+k)} = \\mathbf{w}_i^{(t)} \\times (1 - \\eta \\lambda)^k\n",
    "$\n",
    "\n",
    "This equation shows that $\\mathbf{w}_i$ decays exponentially with each update, with the rate of decay determined by the product of the step size $\\eta$ and the regularization parameter $\\lambda$.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922845f0",
   "metadata": {},
   "source": [
    "f. [3 points] Using your answer in the previous part, come up with an efficient algorithm for regularized SGD when we use a sparse data structure. What is the average time complexity per example?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5167bf7",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "For features not present in the current example (i.e., features with a zero value in the sparse representation), we can apply the exponential decay formula derived in (e) directly instead of computing the full gradient each time.\n",
    "\n",
    "1. **Initialization**: Initialize weights $\\mathbf{w}_i^{(t)}$.\n",
    "2. **For each example** $(\\mathbf{x}^{(j)}, y^{(j)})$ (This is an SGD iteration):\n",
    "   - **For each non-zero feature $\\mathbf{x}_i^{(t)}$ in $\\mathbf{x}^{(t)}$:**\n",
    "     - Compute the gradient of the logistic loss for feature $i$ and update $\\mathbf{w}_i$ using the regular SGD update rule.\n",
    "   - **Efficient Regularization for Zero Features:**\n",
    "     - For all features $i$ that are zero in $\\mathbf{x}^{(j)}$ and have not been updated in the last $k$ examples:\n",
    "       - Update $\\mathbf{w}_i$ using the exponential decay due to regularization only (as derived in the previous part):\n",
    "         $\\mathbf{w}_i^{(t+k)} = \\mathbf{w}_i^{(t)} \\times (1 - \\eta \\lambda)^k$\n",
    "       - This can be done efficiently by keeping track of the last update time for each weight and applying the decay formula only when the weight is accessed.\n",
    "\n",
    "**Time Complexity:**\n",
    "\n",
    "- The average time complexity per example is $O(s)$, where $s$ is the average number of non-zero elements in each example because we only compute gradients and apply updates to the non-zero features in each example.\n",
    "- The regularization for zero-value features can be efficiently handled by updating these weights less frequently, using the exponential decay formula based on the elapsed number of examples since their last update.\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141464e",
   "metadata": {},
   "source": [
    "2. Mixture Discriminant We consider a multi-class classification problem where we predict one out of $K$ classes based on $d$ real-valued features. We use the probabilistic model given by\n",
    "$$\n",
    "P(X \\mid Y=k)=\\sum_{r=1}^{R_k} \\pi_{k r} \\phi\\left(X ; \\mu_{k r}, \\mathbf{C}\\right)\n",
    "$$\n",
    "\n",
    "Here, the function $\\phi(x ; \\mu, \\mathbf{C})$ denotes a Gaussian density with mean $\\mu$ and covariance matrix $\\mathbf{C}$, evaluated in $x$. Thus, the class conditional for each class $k$ is given by a Gaussian mixture with $R_k$ mixture components, weights given by $\\pi_{k:}$, means given by $\\mu_{k:}$, and covariance matrix $\\mathbf{C}$.\n",
    "\n",
    "a. [2 pts] Use Bayes rule to derive the class-posterior probabilities as\n",
    "$$\n",
    "P(Y=k \\mid X=x)=\\frac{\\sum_{r=1}^{R_k} \\pi_{k r} \\phi\\left(X ; \\mu_{k r}, \\mathbf{C}\\right) \\Pi_k}{\\sum_{l=1}^K \\sum_{r=1}^{R_l} \\pi_{l r} \\phi\\left(X ; \\mu_{l r}, \\mathbf{C}\\right) \\Pi_l},\n",
    "$$\n",
    "with the $\\left\\{\\pi_k\\right\\}_{1 \\leq k \\leq K}$ denoting the prior on the relative abundance of the different classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ca9d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f73bd13",
   "metadata": {},
   "source": [
    "b. [2 pts] Formulate an expectation-maximization algorithm for computing the maximum likelihood estimator of (1), given training data $X, Y \\in \\mathbb{R}^{N \\times d} \\times\\{1, \\ldots K\\}^N$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b93d05",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "### Model Overview\n",
    "\n",
    "Our model is a Gaussian Mixture Model (GMM) for each class in a classification setting. The probability density function for each class $k$ is a mixture of Gaussian distributions, where each Gaussian has its own mean $\\mu_{kr}$, the same covariance matrix $\\mathbf{C}$ for all components within a class, and mixture weights $\\pi_{kr}$.\n",
    "\n",
    "The posterior probability of class $k$ given feature vector $X$ is computed using Bayes' rule as you've described.\n",
    "\n",
    "### The EM Algorithm\n",
    "\n",
    "#### 1. **Initialization:**\n",
    "\n",
    "   - Initialize the parameters $\\pi_{kr}$, $\\mu_{kr}$, and $\\mathbf{C}$.\n",
    "     - $\\pi_{kr}$ can be initialized uniformly or based on prior knowledge.\n",
    "     - $\\mu_{kr}$ can be initialized randomly or using methods like k-means clustering.\n",
    "     - $\\mathbf{C}$ can be initialized as the covariance matrix of the entire dataset or each individual class.\n",
    "\n",
    "#### 2. **Expectation Step (E-step):**\n",
    "\n",
    "   - Calculate the responsibility $\\gamma_{ikr}$ that component $r$ of class $k$ has for data point $i$. This is done using the current parameter estimates:\n",
    "     $$\n",
    "     \\gamma_{ikr} = \\frac{\\pi_{kr} \\phi\\left(X_i ; \\mu_{kr}, \\mathbf{C}\\right)}{\\sum_{l=1}^K \\sum_{s=1}^{R_l} \\pi_{ls} \\phi\\left(X_i ; \\mu_{ls}, \\mathbf{C}\\right)}\n",
    "     $$\n",
    "\n",
    "#### 3. **Maximization Step (M-step):**\n",
    "\n",
    "   - Update the parameters $\\pi_{kr}$, $\\mu_{kr}$, and $\\mathbf{C}$ using the responsibilities calculated in the E-step.\n",
    "     - Update means $\\mu_{kr}$:\n",
    "       $$\n",
    "       \\mu_{kr} = \\frac{\\sum_{i=1}^N \\gamma_{ikr} X_i}{\\sum_{i=1}^N \\gamma_{ikr}}\n",
    "       $$\n",
    "     - Update covariance matrix $\\mathbf{C}$:\n",
    "       $$\n",
    "       \\mathbf{C} = \\frac{\\sum_{k=1}^K \\sum_{r=1}^{R_k} \\sum_{i=1}^N \\gamma_{ikr} (X_i - \\mu_{kr})(X_i - \\mu_{kr})^T}{\\sum_{k=1}^K \\sum_{r=1}^{R_k} \\sum_{i=1}^N \\gamma_{ikr}}\n",
    "       $$\n",
    "     - Update weights $\\pi_{kr}$:\n",
    "       $\\pi_{kr} = \\frac{\\sum_{i=1}^N \\gamma_{ikr}}{N}$\n",
    "\n",
    "#### 4. **Convergence Check:**\n",
    "\n",
    "   - Check for convergence (e.g., changes in log-likelihood are below a threshold).\n",
    "   - If not converged, return to step 2.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c533b",
   "metadata": {},
   "source": [
    "c. $[2 \\mathrm{pts}]$ Consider instead the following model, prescribed through it's joint distribution\n",
    "$$\n",
    "P(X, Y)=\\sum_{r=1}^R \\pi_r P_r(Y) \\phi\\left(X ; \\mu_r, \\mathbf{C}\\right) .\n",
    "$$\n",
    "\n",
    "Derive the posterior class distribution as\n",
    "$$\n",
    "P(Y=k \\mid X=x)=\\frac{\\sum_{r=1}^R \\pi_r P_r(Y=k) \\phi\\left(x ; \\mu_r, \\mathbf{C}\\right)}{\\sum_{r=1}^R \\pi_r P_r(Y=k)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58683dd3",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "\n",
    "1. **Marginal Probability $P(X=x)$:**\n",
    "\n",
    "   The marginal probability of $X=x$ is obtained by summing over all possible classes $Y$ in the joint distribution:\n",
    "\n",
    "   $$P(X=x) = \\sum_{Y} P(X=x, Y) = \\sum_{r=1}^R \\pi_r \\phi\\left(x ; \\mu_r, \\mathbf{C}\\right) \\sum_{Y} P_r(Y)$$\n",
    "\n",
    "2. **Class-conditional Probability $P(X=x | Y=k)$:**\n",
    "\n",
    "   The class-conditional probability is the probability of $X=x$ given $Y=k$, which can be expressed using the joint distribution:\n",
    "\n",
    "   $$P(X=x | Y=k) = \\frac{P(X=x, Y=k)}{P(Y=k)} = \\frac{\\sum_{r=1}^R \\pi_r P_r(Y=k) \\phi\\left(x ; \\mu_r, \\mathbf{C}\\right)}{P(Y=k)}$$\n",
    "\n",
    "3. **Posterior Probability $P(Y=k | X=x)$:**\n",
    "\n",
    "   Applying Bayes' Theorem, and using the expressions for $P(X=x | Y=k)$ and $P(X=x)$:\n",
    "\n",
    "   $$P(Y=k | X=x) = \\frac{P(X=x | Y=k) \\cdot P(Y=k)}{P(X=x)}$$\n",
    "   $$P(Y=k | X=x) = \\frac{\\sum_{r=1}^R \\pi_r P_r(Y=k) \\phi\\left(x ; \\mu_r, \\mathbf{C}\\right)}{\\sum_{r=1}^R \\pi_r \\phi\\left(x ; \\mu_r, \\mathbf{C}\\right) \\sum_{Y} P_r(Y)}$$\n",
    "\n",
    "There seems to be a small discrepancy in the formula for the posterior class distribution. It should be derived as follows, considering the normalization factor properly:\n",
    "\n",
    "$$P(Y=k | X=x) = \\frac{\\sum_{r=1}^R \\pi_r P_r(Y=k) \\phi\\left(x ; \\mu_r, \\mathbf{C}\\right)}{\\sum_{r=1}^R \\pi_r \\phi\\left(x ; \\mu_r, \\mathbf{C}\\right)}$$\n",
    "\n",
    "This corrects the denominator to sum over all mixture components rather than just the class-specific components. The denominator is the total probability of observing $X=x$, summed over all mixture components, which normalizes the probability to ensure it sums to 1 across all classes.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9606c25",
   "metadata": {},
   "source": [
    "d. $[2$ pts] Derive the class conditional $P(X \\mid Y)$ for (3) and show that the associated model is a generalization of (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77e4fad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad36d57b",
   "metadata": {},
   "source": [
    "e. $[2 \\mathrm{pts}]$ Derive the EM algorithm for (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4ab7b2",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "#### 1. Initialization\n",
    "\n",
    "- Initialize the parameters $\\pi_r$, $\\mu_r$, and $\\mathbf{C}$.\n",
    "- Initialize $P_r(Y=k)$ for each class $k$ in each mixture component $r$.\n",
    "\n",
    "#### 2. Expectation Step (E-step)\n",
    "\n",
    "- Compute the responsibility $\\gamma_{ir}$ that component $r$ has for data point $i$:\n",
    "  \n",
    "  $$\\gamma_{ir} = \\frac{\\pi_r \\phi(X_i ; \\mu_r, \\mathbf{C})}{\\sum_{s=1}^R \\pi_s \\phi(X_i ; \\mu_s, \\mathbf{C})}$$\n",
    "\n",
    "- Compute the expected class probabilities for each component and data point:\n",
    "\n",
    "  $$\\tau_{irk} = \\frac{P_r(Y=k) \\gamma_{ir}}{\\sum_{k=1}^K P_r(Y=k) \\gamma_{ir}}$$\n",
    "\n",
    "  where $\\tau_{irk}$ represents the probability that data point $i$ belongs to class $k$ in component $r$.\n",
    "\n",
    "#### 3. Maximization Step (M-step)\n",
    "\n",
    "- Update the parameters based on the responsibilities and expected class probabilities:\n",
    "\n",
    "  - Update means $\\mu_r$:\n",
    "    \n",
    "    $$\\mu_r = \\frac{\\sum_{i=1}^N \\gamma_{ir} X_i}{\\sum_{i=1}^N \\gamma_{ir}}$$\n",
    "\n",
    "  - Update covariance matrix $\\mathbf{C}$:\n",
    "    \n",
    "    $$\\mathbf{C} = \\frac{\\sum_{r=1}^R \\sum_{i=1}^N \\gamma_{ir} (X_i - \\mu_r)(X_i - \\mu_r)^T}{\\sum_{r=1}^R \\sum_{i=1}^N \\gamma_{ir}}$$\n",
    "\n",
    "  - Update mixture weights $\\pi_r$:\n",
    "    \n",
    "    $$\\pi_r = \\frac{\\sum_{i=1}^N \\gamma_{ir}}{N}$$\n",
    "\n",
    "  - Update class probabilities $P_r(Y=k)$:\n",
    "    \n",
    "    $$P_r(Y=k) = \\frac{\\sum_{i=1}^N \\tau_{irk}}{\\sum_{i=1}^N \\gamma_{ir}}$$\n",
    "\n",
    "#### 4. Convergence Check\n",
    "\n",
    "- Check if the algorithm has converged (e.g., if the change in log-likelihood is below a certain threshold).\n",
    "- If not converged, repeat from step 2.\n",
    "\n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781a06da",
   "metadata": {},
   "source": [
    "## 3. Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0d00a",
   "metadata": {},
   "source": [
    "The soft margin SVM is formulated as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min _{w \\in \\mathbb{R}^n, b \\in \\mathbb{R}, s \\in \\mathbb{R}^m} & \\frac{1}{2} w^{\\top} w+C \\mathbf{1}^{\\top} s \\\\\n",
    "\\text { s.t. } & X^{\\top} w+b y+s-\\mathbf{1} \\geq \\mathbf{0}, \\\\\n",
    "& s \\geq \\mathbf{0} .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "(1) Let $\\{x_i, y_i\\}_{i=1}^m$ with $x_i \\in \\mathbb{R}^n$ and $y_i \\in\\{ \\pm 1\\}, i \\in[1: m]$, be a training dataset. For a fixed value of $C$, let the corresponding SVM classifier have parameters $w^*, b^*$.\n",
    "\n",
    "(a) Let $h \\in \\mathbb{R}^n$ and $Q \\in \\mathcal{O}_n$ ( $Q$ is an $n \\times n$ matrix), and form the second training set: $\\{Q(x_i-h), y_i\\}_{i=1}^m$. Show that the SVM classifier for this second dataset using the same value of $C$ has parameters $Q w^*, w^{* \\top} h+b^*$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e716938d",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "In SVM, the decision boundary is given by $f(x) = w^{\\top} x + b$.\n",
    "\n",
    "Let $z_i = Q(x_i-h)$ be the new feature, and \n",
    "\n",
    "Let the classifier be $f'(z) = w'^{\\top}z + b'$, where $w'$ and $b'$ are the parameters for the new SVM.\n",
    "\n",
    "$f'(z_i) = w'^{\\top}z_i + b' = w'^{\\top}(Q(x_i-h)) + b'$\n",
    "\n",
    "Since both classifiers make the same decisions for each transformed and original feature pair, we can equate $f'(z_i)$ and $f'(x_i)$:\n",
    "\n",
    "$w^{*\\top}x_i + b^{*} = w'^{\\top}(Q(x_i-h)) + b'$\n",
    "\n",
    "For **Parameter $w'$**:\n",
    "\n",
    "$w'^\\top (Q(x_i - h)) = w'^\\top Q x_i - w'^\\top Q h$.\n",
    "   To make this equal to $w^{*\\top} x_i$, we need $w'^\\top Q = w^{*\\top}$. Thus, $w' = Q w^*$. Here we use the fact that $Q$ is orthogonal ($Q^{\\top} Q = I$)\n",
    "    \n",
    "    \n",
    "For **Parameter $b'$**:\n",
    "\n",
    "We also need to satisfy $b^* = -w'^\\top Q h + b'$. Using $w' = Q w^*$, we have $b^* = -w^{*\\top} Q^\\top Q h + b'$. Since $Q$ is orthogonal ($Q \\in \\mathcal{O}_n$), $Q^\\top Q = I$, and this simplifies to $b^* = -w^{*\\top} h + b'$. Hence, $b' = w^{*\\top} h + b^*$.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ba63e3",
   "metadata": {},
   "source": [
    "(b) If we first center the training examples, how does this change the SVM classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8fd1e4",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "For each training example $x_i$, the centered datapoint is $x_i' = x_i - \\mu$, where $\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x_i$.\n",
    "    \n",
    "\n",
    "After centering, the decision function becomes $f(x') = w^{\\top} x' + b'$.\n",
    "\n",
    "In matrix format\n",
    "    \n",
    "    \n",
    "$$\n",
    "(X - \\mathbf{1} \\bar{x}^{\\top})^{\\top} w+b y+s-\\mathbf{1} \\geq \\mathbf{0}\n",
    "$$\n",
    "    \n",
    "Simplifying:\n",
    "\n",
    "$$\n",
    "X^{\\top} w - \\bar{x}^{\\top} w \\mathbf{1} + b y + s - \\mathbf{1} \\geq \\mathbf{0}\n",
    "$$\n",
    "    \n",
    "\n",
    "The constraints in the SVM optimization problem now applied to the centered data. The constraint $y_i(w^\\top x_i + b) \\geq 1 - \\xi_i$ becomes $y_i(w^\\top (x_i - \\mu) + b') \\geq 1 - \\xi_i$.\n",
    "\n",
    "To find the new bias $b'$, consider the decision function for a support vector $x_i$: $w^\\top x_i + b = 1$ for support vectors on one margin, and $w^\\top x_i + b = -1$ for those on the other. \n",
    "After centering, this becomes $w^\\top (x_i - \\mu) + b' = 1$ or $w^\\top (x_i - \\mu) + b' = -1$, equivalently, $w^\\top x_i = w^\\top \\mu - b' + 1$ or $w^\\top x_i = w^\\top \\mu - b' - 1$. This adjustment can be computed after training the SVM on the centered data.\n",
    "\n",
    "In conclusion, centering the training examples affects the bias term $b$ of the SVM classifier but does not change the weight vector $w$. The change in the bias term compensates for the shift in the data's mean, ensuring that the decision boundary correctly classifies the centered data.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f692bae8",
   "metadata": {},
   "source": [
    "(2) Suppose that instead of using $C \\sum_{i=1}^m s_i$ as the penalty term in the objective of the primal SVM problem we use the quadratic penalty $\\frac{1}{2} C \\sum_{i=1}^m s_i^2$, while maintaining the constraint $s_i \\geq 0$.\n",
    "\n",
    "(a) Formulate the new primal problem in vector form. When is the primal problem feasible?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d1041",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "The new problem with the quadratic penalty term:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min _{w \\in \\mathbb{R}^n, b \\in \\mathbb{R}, s \\in \\mathbb{R}^m} & \\frac{1}{2} w^{\\top} w+C \\sum_{i=1}^{m} s_i^2 \\\\\n",
    "\\text { s.t. } & X^{\\top} w+b y+s-\\mathbf{1} \\geq \\mathbf{0}, \\\\\n",
    "& s \\geq \\mathbf{0} .\n",
    "\\end{aligned}\n",
    "$$\n",
    "    \n",
    "    \n",
    "**Primal Form**:\n",
    "    \n",
    "$\\min L_P = \\frac{1}{2} w^{\\top} w + \\frac{1}{2} C s^{\\top} s + \\sum_i \\alpha_i (1 - w^{\\top}x_i - b y_i - s)$\n",
    "    \n",
    "s.t. $\\forall i, \\alpha_i \\geq 0$ and $\\mathbf{s} \\ge 0$\n",
    "\n",
    "Converting to vector form:\n",
    "\n",
    "$\\min L_P = \\frac{1}{2} w^{\\top} w + \\frac{1}{2} C s^{\\top} s + \\mathbf{\\alpha}^{\\top} (1 - X^{\\top}\\mathbf{w} - b \\mathbf{y} - \\mathbf{s})$\n",
    "    \n",
    "s.t. $\\forall i, \\alpha_i \\geq 0$ and $\\mathbf{s} \\ge 0$\n",
    "    \n",
    "\n",
    "Regarding the feasibility of the primal problem:\n",
    "\n",
    "**Constraint Feasibility**: The primal problem is feasible if there exists at least one set of values $w, b, s$ that satisfies the constraints. In this context, feasibility means that it is possible to find a margin that separates the classes to some extent, allowing for some misclassification or slack as per the $s_i \\geq 0$ constraint. The problem is always feasible since we can always choose $s$ large enough to satisfy the constraints.\n",
    "\n",
    "**Boundedness of the Objective Function**: The objective function is bounded below by 0, as both terms $\\frac{1}{2} w^{\\top} w$ and $\\frac{1}{2} C s^{\\top} s$ are non-negative. This ensures that the optimization problem does not go to negative infinity and a solution (minimum) exists.\n",
    "    \n",
    "**Smoothness of the Objective Function**: The quadratic penalty term $\\frac{1}{2} C \\sum_{i=1}^m s_i^2$ is smooth (twice continuously differentiable), which can make optimization easier, especially for gradient-based methods.\n",
    "    \n",
    "**Impact on Margin and Support Vectors**: The change in the penalty term can lead to a different set of support vectors and potentially a different margin width, as the balance between maximizing the margin and penalizing misclassifications has changed.\n",
    "\n",
    "The quadratic penalty term $\\frac{1}{2} C s^{\\top} s$ makes the problem different from the standard linear penalty term, as it penalizes the slack variables $s_i$ more heavily when they are large. This could lead to a solution where the SVM tries harder to correctly classify the training points, potentially resulting in a narrower margin if the data is not linearly separable.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8684f52",
   "metadata": {},
   "source": [
    "(b) Does strong duality hold for this problem? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee1adf8",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "**Convexity**: Strong duality generally holds when the primal problem is convex. This is often verified through the convexity of the objective function and the feasibility region (the set of all points that satisfy the constraints of the optimization problem) defined by the constraints.\n",
    "\n",
    "**Objective Function Convexity**: The objective function here, $\\frac{1}{2} w^{\\top} w + \\frac{1}{2} C \\sum_{i=1}^m s_i^2$, is clearly convex. The term $\\frac{1}{2} w^{\\top} w$ is a quadratic form which is convex (it is a quadratic function with a positive semi-definite Hessian, in this case, the identity Hessian), and the sum of squared slack variables, $\\frac{1}{2} C \\sum_{i=1}^m s_i^2$, is also convex as it's a sum of convex functions (squares of the slack variables scaled by a positive constant $C$).\n",
    "\n",
    "**Constraint Convexity**: Linear constraint always form a convex feasible region. The constraints $y_i(w^\\top x_i + b) \\geq 1 - s_i$ and $s_i \\geq 0$ define a convex feasibility region. The first set of constraints are linear in terms of $w$, $b$, and $s$, and hence are convex. The second set of constraints are simple non-negativity constraints on $s_i$, which are also convex.\n",
    "\n",
    "**Slater's Condition**: Strong duality requires that [Slater's condition](https://en.wikipedia.org/wiki/Slater%27s_condition) (or some other constraint qualification) is satisfied. For strong duality to hold in convex problems, Slater's condition must be satisfied. Slater's condition states that there must exist a feasible point where all the inequality constraints are strictly satisfied. For this SVM formulation, as long as there exists at least one data point that is not on the margin (which is typically the case in practice), Slater's condition should be satisfied.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c4ad0c",
   "metadata": {},
   "source": [
    "(c) Write down the KKT conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd1fe3",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "   \n",
    "The Karush-Kuhn-Tucker (KKT) conditions are necessary (and under certain conditions, sufficient) for optimality in a constrained optimization problem. For the modified SVM problem with a quadratic penalty, these conditions include the primal feasibility, dual feasibility, complementary slackness, and the gradient of the Lagrangian vanishing at the optimum. \n",
    "    \n",
    "  \n",
    "The Lagrangian is as follows:\n",
    "    \n",
    "$$\n",
    "L(w, b, s, \\alpha, \\beta) = \\frac{1}{2} w^{\\top} w + \\frac{1}{2} C s^{\\top} s - \\alpha^{\\top}(X^{\\top} w + b y + s - \\mathbf{1}) - \\beta^{\\top} s\n",
    "$$\n",
    "\n",
    "Expanding this, we get:\n",
    "\n",
    "$$\n",
    "L(w, b, s, \\alpha, \\beta) = \\frac{1}{2} w^{\\top} w + \\frac{1}{2} C s^{\\top} s - \\sum_{i=1}^m \\alpha_i (w^{\\top} x_i + b y_i + s_i - 1) - \\sum_{i=1}^m \\beta_i s_i\n",
    "$$\n",
    "     \n",
    "\n",
    "The KKT conditions are as follows:\n",
    "\n",
    "**Primal Feasibility**:\n",
    "$\\min L_P = \\frac{1}{2} w^{\\top} w + \\frac{1}{2} C s^{\\top} s + \\sum_i \\alpha_i (1 - w^{\\top}x_i - b y_i - s)$\n",
    "    \n",
    "s.t. $\\forall i, \\alpha_i \\geq 0$\n",
    "\n",
    "**Dual Feasibility**: Introduce Lagrange multipliers $\\alpha \\geq \\mathbf{0}$ for the constraint $X^{\\top} w + b y + s - \\mathbf{1} \\geq \\mathbf{0}$ and $\\beta \\geq \\mathbf{0}$ for $s \\geq \\mathbf{0}$. The dual feasibility condition requires that these multipliers be non-negative.\n",
    "\n",
    "**Complementary Slackness**:\n",
    "    \n",
    "   $$\n",
    "   \\alpha_i (X_i^{\\top} w + b y_i + s_i - 1) = 0, \\quad \\forall i\n",
    "   $$\n",
    "   $$\n",
    "   \\beta_i s_i = 0, \\quad \\forall i\n",
    "   $$\n",
    "\n",
    "**Stationarity**: The gradient of the Lagrangian with respect to $w, b$ and $s$ vanishes at the optimum. Let $L$ be the Lagrangian. Then, the stationarity conditions are:\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4506a213",
   "metadata": {},
   "source": [
    "\n",
    "$$\\nabla_w \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial w} = w - \\sum_{i=1}^m \\alpha_i x_i = \\mathbf{0} \\rightarrow  w = \\sum_{i=1}^m \\alpha_i x_i$$\n",
    "\n",
    "$$\n",
    "\\nabla_b \\mathcal{L} = \\frac{\\partial \\mathcal{L}}{\\partial b} = \\sum_{i=1}^m \\alpha_i y_i = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_s \\mathcal{L} = Cs - \\alpha - \\beta = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "The KKT conditions are a set of necessary conditions for a solution in nonlinear programming to be optimal. When the problem is convex and satisfies Slater's condition (as is the case with this modified SVM problem), these conditions are also sufficient for optimality.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc90b800",
   "metadata": {},
   "source": [
    "\n",
    "The KKT conditions are:\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial w} = 0 \\rightarrow w = \\sum_i \\alpha_i x_i$\n",
    "\n",
    "$\\frac{\\partial \\mathcal{L}}{\\partial b} = 0 \\rightarrow b = \\sum_i \\alpha_i y_i = 0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c22187",
   "metadata": {},
   "source": [
    "\n",
    "Plugging into $\\mathcal{L}$:\n",
    "\n",
    "$\\mathcal{L}(w, b, \\alpha)=\\frac{1}{2} (\\alpha^{\\top}x)^{\\top} \\alpha^{\\top}x + \\frac{1}{2} C \\sum_{i=1}^m s_i^2 -\\frac{1}{2} \\sum_{i j} \\alpha_i \\alpha_j y_i y_j x_i^T x_j$\n",
    "\n",
    "From the stationarity condition for $s_i$:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial s_i} = C s_i - \\alpha_i - \\beta_i = 0$\n",
    "\n",
    "This implies $\\beta_i = C s_i - \\alpha_i$.\n",
    "\n",
    "Using the complementary slackness condition $\\beta_i s_i = 0$, we have two cases:\n",
    "- If $s_i > 0$, then $\\beta_i = 0$ and hence $\\alpha_i = C s_i$.\n",
    "- If $s_i = 0$, then $\\alpha_i \\leq C s_i = 0$.\n",
    "\n",
    "Thus, $0 \\leq \\alpha_i \\leq C$.\n",
    "\n",
    "\n",
    "More in\n",
    "\n",
    "* [Princeton University COS 495 - Machine Learning Basics Lecture 5: SVM II](https://www.cs.princeton.edu/courses/archive/spring16/cos495/slides/ML_basics_lecture5_SVM_II.pdf)\n",
    "* [An Idiotâ€™s guide to Support vector machines (SVMs) (MIT)](https://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf)\n",
    "* [Lecture 5A - UCL](https://www.gatsby.ucl.ac.uk/~gretton/coursefiles/Slides5A.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511e8a8f",
   "metadata": {},
   "source": [
    "(d) Find the dual problem.\n",
    "\n",
    "<div style=\"color:blue\">\n",
    "\n",
    "The dual function is obtained by substituting the expressions for $w$ and $s_i$ into the simplified Lagrangian and simplifying further. \n",
    "\n",
    "$\\max_\\alpha \\quad -\\frac{1}{2} \\sum_{i,j=1}^m \\alpha_i \\alpha_j y_i y_j x_i^\\top x_j + \\sum_{i=1}^m \\alpha_i$\n",
    "\n",
    "Subject to:\n",
    "- $0 \\leq \\alpha_i \\leq C, \\quad \\forall i = 1, \\ldots, m$\n",
    "- $\\sum_{i=1}^m \\alpha_i y_i = 0$\n",
    "\n",
    "This dual problem focuses on finding the optimal values of the Lagrange multipliers $\\alpha$, which correspond to the support vectors in the SVM context.\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8e10c",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37c422a",
   "metadata": {},
   "source": [
    "## 4. KL divergence\n",
    "\n",
    "In many machine learning problems, we often need to measure the \"distance\" between two probability distributions, such as in the E-step of EM algorithms and variational inference. The Kullback-Leibler (KL) divergence is such a statistical distance, and it measures how one probability distribution differs from another. In this problem, we consider discrete probability distributions, i.e.\n",
    "$$\n",
    "\\mathcal{P}=\\{\\left(p_1, \\ldots, p_n\\right) \\mid \\sum_i^n p_i=1, p_i \\geq 0\\} .\n",
    "$$\n",
    "\n",
    "Now for two probability distributions $p, q \\in \\mathcal{P}$, their KL divergence is defined as\n",
    "$$\n",
    "K L(p \\| q)=-\\sum_{i=1}^n p_i \\log \\frac{q_i}{p_i}\n",
    "$$\n",
    "\n",
    "\n",
    "(1) [2pts] Prove that KL divergence is non-negative, i.e., $K L(p \\| q) \\geq 0$, and $K L(p \\| q)=$ 0 if and only if $p=q$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7013775",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "We can use Gibbs' inequality, which states that for any two probability distributions $p$ and $q$:\n",
    "   \n",
    "   $$\n",
    "   \\sum_{i=1}^n p_i \\log p_i \\geq \\sum_{i=1}^n p_i \\log q_i\n",
    "   $$\n",
    "\n",
    "   This can be rearranged to:\n",
    "   \n",
    "   $$\n",
    "   \\sum_{i=1}^n p_i \\log \\frac{p_i}{q_i} \\geq 0\n",
    "   $$\n",
    "\n",
    "   which is the definition of KL divergence. Therefore, $KL(p \\| q) \\geq 0$.\n",
    "\n",
    "The equality in Gibbs' inequality (and also in KL divergence) holds if and only if $p_i = q_i, \\forall i$. That is, $KL(p \\| q) = 0$ if and only if $p = q$.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da607e",
   "metadata": {},
   "source": [
    "(2) [1pt] Is KL divergence symmetric or asymmetric (i.e., is it true that $K L(p \\| q)=$ $K L(q \\| p))$ ? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc8b397",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "$$\n",
    "K L(p \\| q)=-\\sum_{i=1}^n p_i \\log \\frac{q_i}{p_i}\n",
    "$$\n",
    "\n",
    "$$\n",
    "K L(q \\| p)=-\\sum_{i=1}^n q_i \\log \\frac{p_i}{q_i} = \\sum_{i=1}^n q_i \\log \\frac{q_i}{p_i}\n",
    "$$\n",
    "\n",
    "When $K L(p \\| q) = K L(q \\| p)$, we have\n",
    "\n",
    "$K L(p \\| q) - K L(q \\| p) = -\\sum_{i=1}^n (p_i + q_i) \\log \\frac{q_i}{p_i} = 0$\n",
    "\n",
    "This only happens when $q_i = p_i$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e1df66",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "For the left-hand side, Mutual information $I(X, Y)$ is defined as the KL divergence between the joint distribution $p_{X,Y}$ and the product of the marginal distributions $p_X$ and $p_Y$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "I(X, Y) &= KL(p_{X,Y} \\| p_X p_Y) \\\\\n",
    "&= \\sum_{x, y} p_{X,Y}(x, y) \\log \\frac{p_{X,Y}(x, y)}{p_X(x) p_Y(y)} \\\\\n",
    "&= \\sum_{x, y} p_{X,Y}(x, y) \\log p_{X,Y}(x, y) - \\sum_{x, y} p_{X,Y}(x, y) \\log p_X(x) - \\sum_{x, y} p_{X,Y}(x, y) \\log p_Y(y) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For the right-hand side:\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_x p_X(x) \\log p_X(x)\n",
    "$$\n",
    "\n",
    "The conditional entropy $H(X \\mid Y)$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(X \\mid Y) &= -\\sum_{x, y} p_{X,Y}(x, y) \\log p_{X \\mid Y}(x \\mid y) \\\\\n",
    "&= -\\sum_{x, y} p_{X,Y}(x, y) \\log \\frac{p_{X,Y}(x, y)}{p_Y(y)} \\\\\n",
    "&= -\\sum_{x, y} p_{X,Y}(x, y) \\log p_{X,Y}(x, y) - \\sum_{x, y} p_{X,Y}(x, y) p_Y(y)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "H(X) - H(X \\mid Y) = -\\sum_x p_X(x) \\log p_X(x) + \\sum_{x, y} p_{X,Y}(x, y) \\log p_{X,Y}(x, y) + \\sum_{x, y} p_{X,Y}(x, y) p_Y(y)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Notice that the term $\\sum_{x, y} p_{X, Y}(x, y) \\log \\frac{p_{X, Y}(x, y)}{p_Y(y)}$ in $H(X) - H(X \\mid Y)$ can be rewritten to include the $p_X(x)$ term in the log function:\n",
    "\n",
    "$$\n",
    "\\sum_{x, y} p_{X, Y}(x, y) \\log \\frac{p_{X, Y}(x, y)}{p_Y(y)} = \\sum_{x, y} p_{X, Y}(x, y) \\log \\frac{p_{X, Y}(x, y)}{p_X(x) p_Y(y)} + \\sum_{x, y} p_{X, Y}(x, y) \\log p_X(x)\n",
    "$$\n",
    "\n",
    "Since $\\sum_{y} p_{X, Y}(x, y) = p_X(x)$, the additional term simplifies to the negative entropy of $X$:\n",
    "$$\n",
    "\\sum_{x, y} p_{X, Y}(x, y) \\log p_X(x) = \\sum_x p_X(x) \\log p_X(x)\n",
    "$$\n",
    "\n",
    "Therefore, combining these, we have:\n",
    "$$\n",
    "H(X) - H(X \\mid Y) = I(X, Y)\n",
    "$$\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c541a100",
   "metadata": {},
   "source": [
    "(3) [3pts] Consider two random variables $X$ and $Y$ that follow probability distributions $p_X$ and $p_Y$, respectively, and joint distribution $p_{X Y}$. If $X$ and $Y$ are independent, we have $p_{X, Y}=p_X p_Y$. If not, we may be interested in quantifying the degree of their independence. One way to measure this is to consider $K L\\left(p_{X, Y} \\| p_X p_Y\\right)$, which is also known as the mutual information between $X$ and $Y$, denoted as $I(X, Y)$. Prove that\n",
    "\n",
    "$$\n",
    "I(X, Y)=H(X)-H(X \\mid Y)\n",
    "$$\n",
    "\n",
    "where $H(X):=-\\sum_x p_X(x) \\log p_X(x)$ is the entropy of $X$, measuring the uncertainty of $X$, and $H(X \\mid Y):=-\\sum_{x, y} p_{X, Y}(x, y) \\log p_{X \\mid Y}(x \\mid y)$ is the conditional entropy of $X$ given $Y$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f34197",
   "metadata": {},
   "source": [
    "(4) [4pts] Now let us consider a toy machine learning task in generative modeling, where we have a dataset that follows a bimodal distribution $p(X)$, as illustrated in Figure 1. Our goal is to approximate the real distribution $p(X)$ with a model distribution $q_\\theta(X)$. For simplicity, we restrict the $q_\\theta(X)$ to normal distributions, i.e., $q_\\theta(X)=\\mathcal{N}\\left(\\mu, \\sigma^2\\right)$. Since we want to approximate $p(X)$ using $q_\\theta(X)$, a natural objective function is to minimize the KL divergence between these two probability distributions. Here, we have two options for the objective, i.e., $\\min _\\theta K L\\left(p \\| q_\\theta\\right)$ or $\\min _\\theta K L\\left(q_\\theta \\| p\\right)$. For each of the two choices, draw the density curve of $q_\\theta$ that could be obtained by minimizing the corresponding KL divergence, and explain your answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20be40e",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "When we are minimizing $KL(p \\| q_\\theta)$, we are trying to find a model distribution $q_\\theta$ such that the expected log-likelihood under the true distribution $p(X)$ is maximized. This is equivalent to minimizing the expected surprise when $q_\\theta$ is used to approximate $p$. This approach might result in a $q_\\theta$ that is more \"spread out\" to cover the support of $p(X)$ to avoid assigning low probability to any outcome that $p(X)$ might generate.\n",
    "\n",
    "On the other hand, minimizing $KL(q_\\theta \\| p)$ means we are trying to find a model distribution $q_\\theta$ that is as close as possible to the true distribution $p$ in terms of the log-likelihood of $q_\\theta$. This approach tends to produce a $q_\\theta$ that focuses on the modes of $p(X)$, potentially ignoring regions where $p(X)$ has low probability.\n",
    "\n",
    "For a bimodal distribution like the one in Figure 1, minimizing $KL(p \\| q_\\theta)$ would likely result in a $q_\\theta$ that has a broader spread to cover both modes of $p(X)$, even if it means the peak of $q_\\theta$ is not exactly at the peaks of $p(X)$. The density curve of $q_\\theta$ would have to be wider than the individual modes of $p(X)$ to ensure it does not assign low probability to outcomes where $p(X)$ has significant mass.\n",
    "\n",
    "Minimizing $KL(q_\\theta \\| p)$, however, would result in a $q_\\theta$ that closely matches the peaks of $p(X)$ because it's trying to maximize the likelihood of the data that is actually observed. This means $q_\\theta$ may focus on one of the modes of $p(X)$, giving a tighter fit around that mode but possibly ignoring the other mode.\n",
    "\n",
    "In practice, the actual shapes of $q_\\theta$ obtained by minimizing each KL divergence would depend on the initial parameters and the optimization procedure. It's also worth noting that using a single normal distribution to approximate a bimodal distribution is a simplification that may not capture the true nature of $p(X)$. A mixture of Gaussians would typically be a better model for such a distribution. \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab74f2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
