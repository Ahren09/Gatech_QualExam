{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf211f1",
   "metadata": {},
   "source": [
    "## 1. Random forests\n",
    "\n",
    "a. [3 pts] Random forests is a modification over bagging decision trees. The random forests improves variance reduction (over bagging) by reducing correlation among trees. Briefly explain how this correlation reduction (\"de-correlation\") among trees is achieved when growing the trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3f91e",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "1. **Bootstrap Aggregating (Bagging)**: Each tree in a Random Forest is grown on a different bootstrap sample of the data. A bootstrap sample is a randomly selected subset of the data (with replacement), which ensures that each tree is trained on slightly different data. This introduces diversity among the trees.\n",
    "2. **Feature Randomness**: When growing each tree, at each split, only a random subset of features is considered for splitting. This randomness in feature selection further ensures that trees do not just replicate each other, reducing the correlation between them. The combined effect of these two methods is that the trees in a Random Forest capture different aspects and patterns of the data, leading to a reduction in variance without a significant increase in bias.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9d3c9",
   "metadata": {},
   "source": [
    "b. [3 pts] Random forests are generally easy to implement and to train. It can be fit in one sequence, with cross validation performed along the way (almost identical to performing N-fold cross-validation, where N is the number of data instances), through the use of out-of-bag (OOB) samples. Explain why using OOB samples eliminates the need for setting aside a test set for evaluating a random forest, and how this leads to more efficient training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198df62c",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "([Out-of-Bag samples - Wikipedia](https://en.wikipedia.org/wiki/Out-of-bag_error))\n",
    "\n",
    "Out-of-Bag (OOB) samples are data points that are not included in the bootstrap sample used to grow a particular tree. Since Random Forests use bagging, each tree is grown on a different subset of data, and the remaining unused data (OOB) can serve as a test set for that tree.\n",
    "\n",
    "\n",
    "- **Elimination of Separate Test Set**: Each tree is independently validated on its OOB samples, which acts as an internal cross-validation process. This means that every data point gets to be in the OOB sample for some trees and hence gets a chance to be a part of the validation process. This approach negates the need for a separate test set.\n",
    "- **Efficiency**: Since OOB samples are leveraged for validation, the entire dataset is utilized for training as well as validation, leading to more efficient use of data. This is particularly useful in situations where the amount of available data is limited.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f422f3",
   "metadata": {},
   "source": [
    "c. [2 pts] List the model hyperparameters and model parameters of a random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9741f18",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    " - **Model Hyperparameters**: These are the settings adjusted before training to control the behavior of the algorithm. Examples include:\n",
    "   - Number of trees in the forest (`n_estimators`).\n",
    "   - Maximum depth of trees.\n",
    "   - Minimum samples required to split an internal node (`min_samples_split`).\n",
    "   - Minimum samples required at a leaf node (`min_samples_leaf`).\n",
    "   - Number of features to consider when looking for the best split (`max_features`).\n",
    " - **Model Parameters**: These are the learned aspects of the model during training. In Random Forests, these include:\n",
    "   - The structure of each tree (i.e., how splits are made at each node).\n",
    "   - The split points at each node of the trees.\n",
    "   - The feature selected at each split.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677956f",
   "metadata": {},
   "source": [
    "d. [2 pts] Alice and Bob are data scientists debating whether a random forest is an \"interpretable\" model. Alice argues that it is interpretable, while Bob argues that its interpretability is limited. Briefly discuss why they may both be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882587bd",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "\n",
    "Why random forests can be interpretable\n",
    "\n",
    "\n",
    "* Individual Trees: Each tree in a random forest is a simple decision tree, which is inherently interpretable. We can follow the split conditions at each node to understand how a particular prediction was made. We can also easily visualize the model.\n",
    "* Feature Importance: Random forests provide built-in feature importance scores, indicating how heavily each feature influences the final prediction. This helps identify the most relevant features and their potential relationships with the target variable.\n",
    "\n",
    "Why random forests might NOT be interpretable\n",
    "\n",
    "- Ensembling: While individual trees are interpretable, a random forest combines many such trees, making it challenging to interpret the overall decision-making process. The ensemble averaging hides the logic behind specific predictions.\n",
    "- Feature Interactions: Complex interactions between features can occur within a random forest, making it difficult to isolate the independent effect of each feature.\n",
    "- Randomness: the random nature of feature selection and tree generation adds complexity to understanding the model's decisions on a holistic level.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d5883",
   "metadata": {},
   "source": [
    "## 3. Bayesian Linear Regression and Regularization\n",
    "\n",
    "Linear regression is a model of the form $P(y \\mid \\mathbf{x}) \\sim N\\left(\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}, \\sigma^2\\right)$ from a probabilistic point of view, where $\\mathbf{w}$ is a $d$-dimensional vector. In ridge regression, we add an $L2$ regularization term to our least squares objective function to prevent overfitting. Given data $D=\\{\\mathbf{x}_i, y_i\\}_{i=1}^n$, our objective function for ridge regression is then:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w})=\\sum_{i=1}^n\\left(y_i-\\mathbf{w}^{\\mathrm{T}} \\mathbf{x}_i\\right)^2+\\lambda \\mathbf{w}^{\\mathrm{T}} \\mathbf{w} .\n",
    "$$\n",
    "\n",
    "We can arrive at the same objective function in a Bayesian setting, if we consider a maximum a posteriori probability (MAP) estimate and assume $\\mathbf{w}$ has the prior distribution $N(0, f(\\lambda, \\sigma) \\mathbf{I})$.\n",
    "\n",
    "(a) Write down the posterior distribution of $\\mathbf{w}$ given the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aaf18e",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "Reference\n",
    "* [PRML Page 146](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)\n",
    "\n",
    "Given the data $D$ and prior $N(0, f(\\lambda, \\sigma) \\mathbf{I})$ for $w$, the posterior distribution is also normal due to the conjugate prior property\n",
    "\n",
    "\n",
    "* Prior $N(0, f(\\lambda, \\sigma) \\mathbf{I})$: This is our initial belief about the parameters of a model, represented by a probability distribution.\n",
    "* Likelihood: This represents the probability of observing the data given specific parameter values.\n",
    "\n",
    "\n",
    "From Bayes' theorem, the posterior distribution is proportional to the product of the likelihood and the prior. The likelihood function for the given data $D = \\{\\mathbf{x}_i, y_i\\}_{i=1}^n$ under the assumption of independence is:\n",
    "\n",
    "$$\n",
    "P(D \\mid \\mathbf{w}) = \\prod_{i=1}^n N(y_i \\mid \\mathbf{w}^\\mathrm{T} \\mathbf{x}_i, \\sigma^2) .\n",
    "$$\n",
    "\n",
    "The prior distribution of $\\mathbf{w}$ is:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{w}) = N(0, f(\\lambda, \\sigma) \\mathbf{I}) .\n",
    "$$\n",
    "\n",
    "The posterior distribution is:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{w} \\mid D) \\propto P(D \\mid \\mathbf{w}) P(\\mathbf{w}) .\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w} | D) \\propto p(D | \\mathbf{w}) p(\\mathbf{w}) = \\prod_{i=1}^n N\\left(y_i | \\mathbf{w}^{\\mathrm{T}} \\mathbf{x}_i, \\sigma^2\\right) N(0, f(\\lambda, \\sigma) \\mathbf{I}) \\\\\n",
    "% = \\exp(- \\frac{1}{2} \\sum_{i} \\frac{(y_i - w^{\\top} x_i)^2}{\\sigma^2} - \\frac{\\mathbf{w}^T\\mathbf{w}}{f(\\lambda, \\sigma)})\n",
    "% \\exp(- \\frac{1}{2} d \\ln (2\\pi \\sigma^2) - \\frac{1}{2} \\ln{\\mathbf{I}})\n",
    "= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - \\mathbf{w}^{\\mathrm{T}} \\mathbf{x}_i)^2}{2\\sigma^2}\\right) \\frac{1}{\\sqrt{(2\\pi)^k [f(\\lambda, \\sigma)]^k}} \\exp\\left(-\\frac{1}{2f(\\lambda, \\sigma)}\\mathbf{x}^\\mathrm{T} \\mathbf{x}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "The PDF of a multivariate normal distribution with mean vector $\\mathbf{\\mu}$ and covariance matrix $\\mathbf{\\Sigma}$ is given by\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x}; \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}) = \\frac{1}{\\sqrt{(2\\pi)^k |\\boldsymbol{\\Sigma}|}} \\exp\\left(-\\frac{1}{2}(\\mathbf{x} - \\boldsymbol{\\mu})^\\mathrm{T} \\boldsymbol{\\Sigma}^{-1} (\\mathbf{x} - \\boldsymbol{\\mu})\\right)\n",
    "$$\n",
    "\n",
    "where $k$ is the dimensionality of $\\mathbf{w}$.\n",
    "\n",
    "Combining the exponentials and simplifying, we get:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{w} | D) \\propto N\\left(\\mathbf{w} | \\hat{\\mathbf{w}}, \\hat{\\Sigma}\\right)\n",
    "$$\n",
    "\n",
    "where the posterior mean $\\mathbf{\\hat{w}}$ and covariance $\\mathbf{\\hat{\\Sigma}}$ are:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{w}} = \\left( \\dfrac{X^\\mathsf{T} X}{n \\sigma^2 + f(\\lambda, \\sigma)} + \\dfrac{\\mathbf{0}}{\\sigma^2} \\right)^{-1} X^\\mathsf{T} y \\\n",
    "\\hat{\\Sigma} = \\left( \\dfrac{X^\\mathsf{T} X}{n \\sigma^2 + f(\\lambda, \\sigma)} + \\dfrac{\\mathbf{I}}{\\sigma^2} \\right)^{-1}\n",
    "$$\n",
    "\n",
    "Therefore, the posterior distribution of $\\hat{\\mathbf{w}}$ given the data is also a Gaussian distribution with mean $\\hat{\\mathbf{w}}$ and covariance $\\hat{\\mathbf{\\Sigma}}$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e228f4",
   "metadata": {},
   "source": [
    "(b) What $f(\\lambda, \\sigma)$ makes this MAP estimate the same as the solution to optimizing $J(\\mathbf{w})$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476444b5",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "\n",
    "In Bayesian linear regression, we are trying to estimate the posterior distribution of the weights $\\mathbf{w}$. We have:\n",
    "\n",
    "- The likelihood $P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}, \\sigma^2)$ which is normally distributed.\n",
    "- The prior $P(\\mathbf{w})$ which is also normally distributed.\n",
    "\n",
    "The likelihood for a linear regression model with Gaussian noise is given by:\n",
    "\n",
    "$P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}, \\sigma^2) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i - \\mathbf{w}^\\mathrm{T}\\mathbf{x}_i)^2\\right)$\n",
    "\n",
    "For ridge regression, we assume a Gaussian prior for $\\mathbf{w}$:\n",
    "\n",
    "$P(\\mathbf{w}) = N(0, \\tau^2\\mathbf{I})$\n",
    "$\\propto \\exp\\left(-\\frac{1}{2\\tau^2}\\mathbf{w}^\\mathrm{T}\\mathbf{w}\\right)$\n",
    "\n",
    "The MAP estimate maximizes the posterior $P(\\mathbf{w} | \\mathbf{X}, \\mathbf{y})$, which is proportional to the product of the likelihood and the prior:\n",
    "\n",
    "$\\log P(\\mathbf{w} | \\mathbf{X}, \\mathbf{y}) \\propto \\log P(\\mathbf{y} | \\mathbf{X}, \\mathbf{w}, \\sigma^2) + \\log P(\\mathbf{w})$\n",
    "\n",
    "Substituting the expressions for the likelihood and the prior, and ignoring constants that do not depend on $\\mathbf{w}$, we get:\n",
    "\n",
    "$\\log P(\\mathbf{w} | \\mathbf{X}, \\mathbf{y}) \\propto -\\frac{1}{2\\sigma^2}\\sum_{i=1}^n(y_i - \\mathbf{w}^\\mathrm{T}\\mathbf{x}_i)^2 - \\frac{1}{2\\tau^2}\\mathbf{w}^\\mathrm{T}\\mathbf{w}$\n",
    "\n",
    "To make this equivalent to the ridge regression objective function $J(\\mathbf{w})$, we need to equate the coefficients of $\\mathbf{w}^\\mathrm{T}\\mathbf{w}$ in the prior term with the regularization term $\\lambda \\mathbf{w}^\\mathrm{T}\\mathbf{w}$ in $J(\\mathbf{w})$. This gives us:\n",
    "\n",
    "$\\frac{1}{2\\tau^2}\\mathbf{w}^\\mathrm{T}\\mathbf{w} = \\lambda \\mathbf{w}^\\mathrm{T}\\mathbf{w}$\n",
    "$\\frac{1}{\\tau^2} = 2\\lambda$\n",
    "$\\tau^2 = \\frac{1}{2\\lambda}$\n",
    "\n",
    "Since we are given that the prior is $N(0, f(\\lambda, \\sigma)\\mathbf{I})$, it implies:\n",
    "\n",
    "$f(\\lambda, \\sigma) = \\tau^2$\n",
    "$f(\\lambda, \\sigma) = \\frac{1}{2\\lambda}$\n",
    "\n",
    "Therefore, the function $f(\\lambda, \\sigma) = \\frac{1}{2\\lambda}$ makes the MAP estimate equivalent to the solution of the ridge regression objective function $J(\\mathbf{w})$. Note that in this formulation, $f$ does not depend on $\\sigma$, as the variance of the noise in the likelihood does not directly affect the prior's variance in this specific setup.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878ebec",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Recommendation Systems\n",
    "\n",
    "| | Watched? | | | | | Rated? | | | | |\n",
    "| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n",
    "| | Alice | Bob | Charles | David | Eugene | Alice | Bob | Charles | David | Eugene |\n",
    "| Friends | 1 | 1 | 0 | 1 | 1 | 5 | 3 | ? | 1 | 4 |\n",
    "| The Office | 1 | 0 | 0 | 0 | 1 | 5 | ? | ? | ? | 4 |\n",
    "| Arrested Development (AD) | 1 | 0 | 0 | 0 | 0 | 4 | ? | ? | ? | ? |\n",
    "| The Bing Bang Theory (BBT) | 0 | 1 | 0 | 0 | 0 | ? | 2 | ? | ? | ? |\n",
    "| The Marvelous Mrs. Maisel (MMM) | 1 | 0 | 1 | 1 | 1 | 1 | ? | 1 | 2 | 4 |\n",
    "\n",
    "Recommendation Systems\n",
    "You have collected the following ratings of popular comedy TV shows from five users:\n",
    "\n",
    "\n",
    "(a) (4 points) To generate recommendations, you adopt the following policy: if a user U likes item X, then U will also like item Y. You implement this by maximizing the cosine similarity between the ratings of items X and Y. Your policy also states that you will only make a recommendation to user U if (a) U has not already watched or rated Y and (b) U’s rating of item X is at least 3. Using this policy, which TV show would be recommended to Eugene? Show the com- parisons that you made.\n",
    "\n",
    "\n",
    "<div style=\"color:blue\">\n",
    "\n",
    "References\n",
    "\n",
    "* [User-Based Collaborative Filtering (Geeksforgeeks)](https://www.geeksforgeeks.org/user-based-collaborative-filtering/): This calculation applies normalization before calculating cosing similarity, which might not be necessary. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b44ef1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e219d6d",
   "metadata": {},
   "source": [
    "(b) (3 points) Next, you design a recommendation system to rank TV show to find the ‘Best TV Shows of All Times’, using the following formula: ratings(i) = a + b(i). In this formula, you set a as a global term and b(i) as an item’s bias score. You first fit this model to calculate a as the mean of all ratings across the dataset, and in the process, you calculate b(i) to be the remainder value per item.\n",
    "You rank the items according to their bias scores (higher bias score is ranked higher). Which item, among the five shows shown in Table 1, would be the Best TV Show and which one would be the Worst TV show? Show your calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3231946",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f11cf45",
   "metadata": {},
   "source": [
    "\n",
    "(c) (3 points) You come up with the idea of training a deep learning-based recom- mendation system model, namely the Neural Collaborative Filtering (NCF) model, on your large dataset to create better recommendation models. Your large dataset has 10 million ratings given by approximately 100,000 users to approximately 1,000,000 movies.\n",
    "Your NCF model first generates 8-dimensional user and item embeddings. Then you pass the embeddings through two fully-connected neural CF layers with sizes 8x16 and 16x16 dimensions. Finally, this is passed through a 16x1 output layer with ReLU activation to produce a single prediction value of recommending an item to a user. You train the model for 10 epochs with back-propagation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edb6546",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7737df9",
   "metadata": {},
   "source": [
    "## 4. Gaussian statistics\n",
    "\n",
    "You were hired to accompany an expedition to study the legendary mathematodon, an enormous amphibian mammal living exclusively on the shepherd's islands, hundreds of nautical miles southwest of Australia. After arriving on the archipelago, you begin collecting data at each adult mathematodon sighting, including the size of its hoofs and its height. After collecting a large number $N$ of measurements, you gather them into an $N \\times 2$ matrix $\\mathbf{A}$, with the first column corresponding to the diameter of the mathematodon's forehoofs and the second to its height.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c1fa2e",
   "metadata": {},
   "source": [
    "(a) (1 Point) How would you compute the mean $\\mathbf{m}$ and covariance $\\mathbf{C}$ of the joint distribution of the diameter of the forehoof and height of a mathematodon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894b731",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "**Mean ($\\mathbf{m}$)**: The mean of each feature (column) can be calculated by averaging each column of $\\mathbf{A}$. If $\\mathbf{A}_1$ and $\\mathbf{A}_2$ are the first and second columns respectively, then $\\mathbf{m} = [\\bar{A}_1, \\bar{A}_2]$ where $\\bar{A}_1$ and $\\bar{A}_2$ are the means of the respective columns.\n",
    "\n",
    "$$\n",
    "\\mathbf{m} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{A}_i\n",
    "$$\n",
    "\n",
    "where $\\mathbf{A}_i$ is the $i^{th}$ row of the matrix $\\mathbf{A}$.\n",
    "\n",
    "**Covariance ($\\mathbf{C}$)**: The covariance matrix is a 2x2 matrix representing the covariance between the two features. represents how much the dimensions vary from the mean with respect to each other. In this case, it is a $2 \\times 2$ matrix because there are two dimensions: diameter of the forehoofs and height. It can be computed using the formula $\\mathbf{C} = \\frac{1}{N-1} (\\mathbf{A} - \\mathbf{m})^T(\\mathbf{A} - \\mathbf{m})$.\n",
    "\n",
    "The covariance matrix is calculated as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\frac{1}{N-1} \\sum_{i=1}^{N} (\\mathbf{A}_i - \\mathbf{m})^T (\\mathbf{A}_i - \\mathbf{m})\n",
    "$$\n",
    "\n",
    "Here, $(\\mathbf{A}_i - \\mathbf{m})$ represents the deviation of each measurement from the mean, and the transpose operation $(\\cdot)^T$ is used because we are multiplying a column vector by a row vector to get a matrix.\n",
    "\n",
    "The $\\frac{1}{N-1}$ term is used instead of $\\frac{1}{N}$ for an unbiased estimate of the covariance in a sample covariance calculation.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b46455d",
   "metadata": {},
   "source": [
    "(b) (3 Points) You discover the imprint of a mathematodon forehoof of diameter d. You were unable to observe the mathematodon itself, but assume that hoof size and height are jointly Gaussian. Use the maximum likelihood criterion to estimate the parameters of their joint Gaussian distribution. What is the optimal estimate for the height of the unseen mathematodon (as a function of $\\mathbf{A}$ and $d$). What is the mean squared error of this estimate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2877b",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "1. **Calculate the Sample Mean of Forehoof Diameters and Heights:**\n",
    "\n",
    "   Let $\\bar{d}$ be the mean diameter of the forehoofs, and $\\bar{h}$ the mean height.\n",
    "\n",
    "   $\\bar{d} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{A}_{i,1}$\n",
    "   \n",
    "   $\\bar{h} = \\frac{1}{N} \\sum_{i=1}^{N} \\mathbf{A}_{i,2}$\n",
    "\n",
    "\n",
    "2. **Calculate the Sample Covariances:**\n",
    "\n",
    "    $S_{dd} = \\frac{1}{N-1} \\sum_{i=1}^{N} (\\mathbf{A}_{i,1} - \\bar{d})^2$\n",
    "\n",
    "    $S_{dh} = S_{hd} = \\frac{1}{N-1} \\sum_{i=1}^{N} (\\mathbf{A}_{i,1} - \\bar{d})(\\mathbf{A}_{i,2} - \\bar{h})$\n",
    "   \n",
    "    $S_{hh} = \\frac{1}{N-1} \\sum_{i=1}^{N} (\\mathbf{A}_{i,2} - \\bar{h})^2$\n",
    "\n",
    "   Here, $S_{dd}$ is the variance of forehoof diameters, $S_{hh}$ is the variance of heights, and $S_{dh} = S_{hd}$ is the covariance between diameters and heights.\n",
    "\n",
    "3. **Estimate the Height for the Observed Diameter $d$:**\n",
    "\n",
    "   The conditional expectation of height given the diameter $d$ is:\n",
    "\n",
    "   $$\n",
    "   \\hat{h} = \\bar{h} + \\frac{S_{hd}}{S_{dd}} (d - \\bar{d})\n",
    "   $$\n",
    "\n",
    "   The height $h$ given $d$ can be estimated using the formula:\n",
    "\n",
    "  $$\n",
    "  h = m_2 + C_{21}C_{11}^{-1}(d - m_1)\n",
    "  $$\n",
    "\n",
    "4. **Calculate the Mean Squared Error (MSE) of the Estimate:**\n",
    "\n",
    "    The MSE of this estimation, given the Gaussian assumption, is the conditional variance:\n",
    "\n",
    "    $\\text{MSE} = S_{hh} - \\frac{S_{hd}^2}{S_{dd}}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "<text style=\"color:red\">\n",
    "\n",
    "\n",
    "\n",
    "More about Joint Gaussian\n",
    "\n",
    "- [Multivariate Gaussian](https://people.eecs.berkeley.edu/~jordan/courses/260-spring10/other-readings/chapter13.pdf)\n",
    "\n",
    "The PDF for multivariate Gaussian:\n",
    "\n",
    "$p(x \\mid \\mu, \\Sigma)=\\frac{1}{(2 \\pi)^{n / 2}|\\Sigma|^{1 / 2}} \\exp \\{-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1}(x-\\mu)\\}$\n",
    "\n",
    "$\\mu = E(x)$\n",
    "\n",
    "$\\Sigma = E(x - \\mu) E(x - \\mu)^{\\top}$\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c3703a",
   "metadata": {},
   "source": [
    "(c) (1 Points) In the last problem, would it have been enough to assume that the two variables are marginally Gaussian to arrive at the same conclusion? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459602e",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "\n",
    "- Two variables are **marginally Gaussian** means that each variable, considered separately, follows a Gaussian distribution. However, this does not imply any specific form of the joint distribution of these two variables. Knowing that each variable is marginally Gaussian tells us about the individual distributions of forehoof diameter and height, but it does not provide information about how these two variables are related or co-vary with each other.\n",
    "- **Joint Gaussianity** is a stronger assumption. It not only implies that each variable is marginally Gaussian but also that their joint distribution is Gaussian. It implies certain linear relationships between the variables (captured in the covariance matrix). This joint Gaussianity allows us to make specific statements about the relationship between the variables, including the ability to derive the conditional distribution of one variable given the other. This is crucial for estimating the height given the forehoof diameter using the conditional expectation formula derived earlier. In the case of jointly Gaussian variables, the conditional distribution of one variable given the other is still Gaussian, and its mean and variance can be expressed in terms of the means, variances, and covariance of the two variables.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "\n",
    "\n",
    "- Simply assuming that the two variables are marginally Gaussian would not have been sufficient to arrive at the same conclusion. While this assumption would tell us about the individual behaviors of forehoof diameter and height, it would not provide information about their joint behavior or the conditional relationships between them.\n",
    "- To estimate the height from the forehoof diameter using the methods described earlier (such as conditional expectation and mean squared error), we need the assumption of joint Gaussianity. This assumption allows us to exploit the properties of the Gaussian distribution in multi-dimensional space, particularly the nature of the conditional distributions.\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "<text style=\"color:red\">\n",
    "\n",
    "Example of Joint Gaussian: The joint Gaussian distribution is like a sophisticated guide that helps you guess the height and weight of a people at the same time. It tells you not just the average height and weight (like the average height might be 5'9\" and the average weight 150 lbs), but also how these two are related, e.g. taller people tend to be heavier. Joint Gaussian distribution describes how two or more variables (like height and weight) are related to each other in terms of their averages (mean) and how much they vary together (covariance). \n",
    "\n",
    "Example of Marginal Gaussian: only guess the height of people, not caring about their weight. It's like taking the joint Gaussian distribution (which has both height and weight) and simplifying it to consider just one variable (height). It still follows a Gaussian (bell curve) pattern but is focused on a single variable and ignore the others.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18158a93",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d341fea",
   "metadata": {},
   "source": [
    "(d) (2 Points) The Bergmann's rule suggests that the variability of both hoof size and height varies from north to south. For $s \\in[-1,1]$ denoting the north-south position within the archipelago, consider the parametric form of the Bregmann rule given by the mean $\\mathbf{m}$ and covariance $\\mathbf{C}+\\alpha s \\mathbf{I}$, where $\\mathbf{I}$ is the identity matrix and $\\alpha \\in \\mathbb{R}$ is a parameter. Let the vector $\\mathbf{s} \\in \\mathbb{R}^N$ contain the $s$-values associated to the data collected in $\\mathbf{A}$. What is the range of values for $\\alpha$ that specify a valid Gaussian Process model throughout the entire archipelago? Formulate a maximum likelihood criterion for determining $\\alpha$.\n",
    "\n",
    "<div style=\"color:blue\">\n",
    "\n",
    "### Determining the Valid Range for $\\alpha$\n",
    "\n",
    "1. **Condition for Positive Semi-Definiteness:**\n",
    "   The covariance matrix $\\mathbf{C} + \\alpha s \\mathbf{I}$ must be positive semi-definite. This condition is satisfied if all eigenvalues of $\\mathbf{C} + \\alpha s \\mathbf{I}$ are non-negative.\n",
    "\n",
    "2. **Eigenvalues of $\\mathbf{C}$:**\n",
    "   Let $\\lambda_1, \\lambda_2, \\ldots, \\lambda_n$ be the eigenvalues of $\\mathbf{C}$. Since $\\mathbf{C}$ is a covariance matrix, we have $\\lambda_i \\geq 0$ for all $i$.\n",
    "\n",
    "3. **Eigenvalues with Modification:**\n",
    "   The eigenvalues of $\\mathbf{C} + \\alpha s \\mathbf{I}$ will be $\\lambda_i + \\alpha s$. To ensure these are non-negative for all $s \\in [-1, 1]$ and all $i$, we need:\n",
    "\n",
    "   $\n",
    "   \\lambda_i + \\alpha s \\geq 0, \\quad \\forall s \\in [-1, 1]\n",
    "   $\n",
    "\n",
    "4. **Range of $\\alpha$:**\n",
    "   Given $s \\in [-1, 1]$, the most restrictive condition occurs when $s = -1$. Therefore, we require:\n",
    "\n",
    "   $\n",
    "   \\lambda_i - \\alpha \\geq 0 \\quad \\Rightarrow \\quad \\alpha \\leq \\lambda_i, \\quad \\forall i\n",
    "   $\n",
    "\n",
    "   Since this must hold for all eigenvalues, the maximum allowable $\\alpha$ is:\n",
    "\n",
    "   $\\alpha \\leq \\min_i(\\lambda_i)$\n",
    "\n",
    "### Maximum Likelihood Criterion for $\\alpha$\n",
    "\n",
    "1. **Log-Likelihood Function:**\n",
    "   The log-likelihood of the Gaussian process model, given data $\\mathbf{A}$ and positions $\\mathbf{s}$, is:\n",
    "\n",
    "   $\n",
    "   \\log L(\\alpha) = -\\frac{1}{2} \\sum_{i=1}^{N} \\left[ \\log |\\mathbf{C} + \\alpha s_i \\mathbf{I}| + (\\mathbf{A}_i - \\mathbf{m})^T (\\mathbf{C} + \\alpha s_i \\mathbf{I})^{-1} (\\mathbf{A}_i - \\mathbf{m}) \\right]\n",
    "   $\n",
    "\n",
    "   where $\\mathbf{A}_i$ is the $i$-th row of $\\mathbf{A}$ and $s_i$ is the $i$-th element of $\\mathbf{s}$.\n",
    "\n",
    "2. **Maximize Log-Likelihood:**\n",
    "   To find the optimal $\\alpha$, differentiate the log-likelihood with respect to $\\alpha$ and set it to zero:\n",
    "\n",
    "   $\n",
    "   \\frac{\\partial \\log L(\\alpha)}{\\partial \\alpha} = 0\n",
    "   $\n",
    "\n",
    "   Solving this equation will give you the value of $\\alpha$ that maximizes the likelihood. This typically requires numerical methods as the solution might not have a closed-form expression.\n",
    "\n",
    "By following these steps with the specific data $\\mathbf{A}$ and $\\mathbf{s}$, you can determine the valid range for $\\alpha$ and find the maximum likelihood estimate of $\\alpha$ for your Gaussian Process model.\n",
    "\n",
    "\n",
    "\n",
    "Given the log-likelihood function:\n",
    "\n",
    "$\n",
    "\\log L(\\alpha) = -\\frac{1}{2} \\sum_{i=1}^{N} \\left[ \\log |\\mathbf{C} + \\alpha s_i \\mathbf{I}| + (\\mathbf{A}_i - \\mathbf{m})^T (\\mathbf{C} + \\alpha s_i \\mathbf{I})^{-1} (\\mathbf{A}_i - \\mathbf{m}) \\right]\n",
    "$\n",
    "\n",
    "We differentiate this with respect to $\\alpha$:\n",
    "\n",
    "$\n",
    "\\frac{\\partial \\log L(\\alpha)}{\\partial \\alpha} = -\\frac{1}{2} \\sum_{i=1}^{N} \\left[ \\frac{\\partial}{\\partial \\alpha} \\log |\\mathbf{C} + \\alpha s_i \\mathbf{I}| + \\frac{\\partial}{\\partial \\alpha} \\left( (\\mathbf{A}_i - \\mathbf{m})^T (\\mathbf{C} + \\alpha s_i \\mathbf{I})^{-1} (\\mathbf{A}_i - \\mathbf{m}) \\right) \\right]\n",
    "$\n",
    "\n",
    "The derivative of the first term, $\\log |\\mathbf{C} + \\alpha s_i \\mathbf{I}|$, can be expressed using the matrix derivative identity:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\alpha} \\log |\\mathbf{C} + \\alpha s_i \\mathbf{I}| = \\text{trace} \\left( (\\mathbf{C} + \\alpha s_i \\mathbf{I})^{-1} s_i \\mathbf{I} \\right)$$\n",
    "\n",
    "The derivative of the second term is more complex due to the inverse matrix inside the expression. It involves the derivative of a matrix inverse and requires applying the chain rule for matrix calculus.\n",
    "\n",
    "</div>\n",
    "\n",
    "(e) (3 Points) After returning from the expedition, your colleagues point out that the forehoof-sizes and heights of male and female mathematodons are likely following different distributions. DARN! Beginner's mistake! You forgot to note down whether you observed male or female mathematodons! Thankfully you still have your raw data A. Describe how you could try to recover the missing information by modeling your data as a Gaussian mixture model and derive an ExpectationMaximization algorithm to fit this model to the data given by $\\mathbf{A}$.\n",
    "\n",
    "<div style=\"color:blue\">\n",
    "    \n",
    "    \n",
    "Given a matrix $\\mathbf{A}$ of size $N \\times 2$, where the first column corresponds to the diameter of the forehoofs and the second to the height of mathematodons:\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "(b) (3 Points) You discover the imprint of a mathematodon forehoof of diameter $d$. You were unable to observe the mathematodon itself, but assume that hoof size and height are jointly Gaussian. Use the maximum likelihood criterion to estimate the parameters of their joint Gaussian distribution. What is the optimal estimate for the height of the unseen mathematodon (as a function of $\\mathbf{A}$ and $d$ ). What is the mean squared error of this estimate?\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"color:blue\">\n",
    "\n",
    "Given a hoof diameter $d$, and assuming a joint Gaussian distribution, we can use the conditional probability formula for Gaussian distributions. \n",
    "\n",
    "where $m_1$ and $m_2$ are the means of the forehoof diameter and height, respectively, $C_{11}$ is the variance of the forehoof diameter, and $C_{21}$ is the covariance between height and forehoof diameter. The mean squared error of this estimate can be derived from the conditional variance formula in Gaussian distributions.\n",
    "\n",
    "\n",
    "**Likelihood function**: Assuming a joint Gaussian distribution with parameters mean = $m$ and covariance = $C$, the likelihood for a single data point $(d, h)$ is:\n",
    "\n",
    "$$\n",
    "P(d, h | m, C) = \\frac{1}{2 \\pi |C|^\\frac{1}{2}} \\exp (- \\frac{1}{2} [\\frac{(d - m_1)^2}{C_{11}} + \\frac{2 * (d - m_1) * (h - m_2)}{C_{12}} + \\frac{(h - m_2)^2}{C_{22}}])\n",
    "$$\n",
    "\n",
    "Mean Squared Error (MSE): Calculate the expected squared difference between the estimated height h and the true height h_true:\n",
    "\n",
    "MSE = E[(h - h_true)^2]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "### Part (d): Validity of Gaussian Process Model with Bregmann Rule\n",
    "\n",
    "For the Gaussian Process model to remain valid, the covariance matrix ($\\mathbf{C}+\\alpha s \\mathbf{I}$) must be positive semi-definite for all $s \\in [-1,1]$. This condition sets constraints on the range of $\\alpha$. To maximize the likelihood for determining $\\alpha$, one would typically maximize the log-likelihood function derived from the Gaussian Process model over the observed data, taking into account the modified covariance matrix.\n",
    "\n",
    "\n",
    "<div style=\"color:blue\">\n",
    "    \n",
    "    \n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0dc343",
   "metadata": {},
   "source": [
    "### Part (e): Gaussian Mixture Model for Sex Determination\n",
    "\n",
    "To recover the missing information about the sex of the mathematodons:\n",
    "\n",
    "1. **Modeling with GMM**: Assume two Gaussian distributions, one for male and one for female mathematodons. Each Gaussian will have its own mean and covariance matrix.\n",
    "\n",
    "2. **Expectation-Maximization (EM) Algorithm**: The EM algorithm will iteratively update the parameters of the two Gaussians and the probabilities of each data point belonging to either distribution. In the Expectation step, calculate the probabilities of each data point belonging to each distribution. In the Maximization step, update the parameters of the Gaussians based on these probabilities. Iterate until convergence.\n",
    "\n",
    "This problem requires a combination of statistical modeling and algorithmic implementation for a comprehensive solution.\n",
    "\n",
    "\n",
    "<div style=\"color:blue\">\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
