{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aadf54d8",
   "metadata": {},
   "source": [
    "## 1 \n",
    "\n",
    "We have two finite sets of items $\\mathcal{U}=\\left\\{u_1, \\ldots, u_M\\right\\}$ and $\\mathcal{V}=\\left\\{v_1, \\ldots, v_N\\right\\}$. For example, $U$ can be a set of adjectives and $V$ be a set of nouns. Let $p\\left(u_i, v_j\\right)$ be the probability that the items $u_i$ and $v_j$ co-occur (for example, a pair of adjective and noun occurs as the subject of a sentence). Assume for some latent variable $z$ taking values from $\\{1, \\ldots, K\\}$, we have the conditional independence,\n",
    "\n",
    "$$\n",
    "p\\left(u_i, v_j \\mid z=k\\right)=p\\left(u_i \\mid z=k\\right) p\\left(v_j \\mid z=k\\right) .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3632dc",
   "metadata": {},
   "source": [
    "(a) Let $P=\\left[p\\left(u_i, v_j\\right)\\right] \\in R^{M \\times N}$, the $M$-by- $N$ matrix whose $(i, j)$ element is $p\\left(u_i, v_j\\right) ; U=\\left[p\\left(u_i \\mid z=\\right.\\right.$ $k)] \\in R^{M \\times K}$, the $M$-by- $K$ matrix whose $(i, k)$ element is $p\\left(u_i \\mid z=k\\right) ; V=\\left[p\\left(v_j \\mid z=k\\right)\\right] \\in R^{N \\times K}$, the $N$-by- $K$ matrix whose $(j, k)$ element is $p\\left(v_j \\mid z=k\\right)$; and the $K$-by- $K$ diagonal matrix $\\Sigma$ with $(k, k)$ element $p(z=k)$. Show that\n",
    "$$\n",
    "P=U \\Sigma V^T .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d909dd",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "\n",
    "To show that $P = U \\Sigma V^T$, where $P$ is the $M \\times N$ matrix of joint probabilities, $U$ and $V$ are matrices of conditional probabilities given $z$, and $\\Sigma$ is a diagonal matrix with the probabilities of $z$, we will use the conditional independence property and the law of total probability.\n",
    "\n",
    "Given: \n",
    "- $\\mathcal{U} = \\{u_1, \\ldots, u_M\\}$ and $\\mathcal{V} = \\{v_1, \\ldots, v_N\\}$.\n",
    "- $p(u_i, v_j \\mid z=k) = p(u_i \\mid z=k) p(v_j \\mid z=k)$.\n",
    "\n",
    "We define the matrices:\n",
    "- $P = [p(u_i, v_j)] \\in \\mathbb{R}^{M \\times N}$,\n",
    "- $U = [p(u_i \\mid z=k)] \\in \\mathbb{R}^{M \\times K}$,\n",
    "- $V = [p(v_j \\mid z=k)] \\in \\mathbb{R}^{N \\times K}$,\n",
    "- $\\Sigma$ as a $K \\times K$ diagonal matrix with $\\Sigma_{kk} = p(z=k)$.\n",
    "\n",
    "To show $P = U \\Sigma V^T$:\n",
    "\n",
    "1. Consider the element $P_{ij}$ in matrix $P$, which is $p(u_i, v_j)$.\n",
    "2. By the law of total probability and the given conditional independence, we can express $p(u_i, v_j)$ as:\n",
    "   $$\n",
    "   p(u_i, v_j) = \\sum_{k=1}^{K} p(u_i, v_j \\mid z=k)p(z=k)\n",
    "   $$\n",
    "3. Using the conditional independence property, we expand $p(u_i, v_j \\mid z=k)$ as $p(u_i \\mid z=k) p(v_j \\mid z=k)$. Thus, the above equation becomes:\n",
    "   $$\n",
    "   p(u_i, v_j) = \\sum_{k=1}^{K} p(u_i \\mid z=k) p(v_j \\mid z=k) p(z=k)\n",
    "   $$\n",
    "4. We now recognize this as a matrix multiplication:\n",
    "   - $U_{ik} = p(u_i \\mid z=k)$,\n",
    "   - $V_{jk} = p(v_j \\mid z=k)$,\n",
    "   - $\\Sigma_{kk} = p(z=k)$.\n",
    "5. Thus, the element $P_{ij}$ is the sum of the products of corresponding elements from $U$, $\\Sigma$, and $V^T$:\n",
    "   $$\n",
    "   P_{ij} = \\sum_{k=1}^{K} U_{ik} \\Sigma_{kk} V^T_{kj}\n",
    "   $$\n",
    "6. This sum over $k$ for each $i, j$ pair is precisely the matrix multiplication $U \\Sigma V^T$.\n",
    "\n",
    "Hence, we have shown that $P = U \\Sigma V^T$.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ef596",
   "metadata": {},
   "source": [
    "(b) How does the above factorization of $P$ differ from the singular value decomposition of $P$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836fa554",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "The factorization of $P$ as $P = U \\Sigma V^T$ in the context of the given problem has some similarities to the Singular Value Decomposition (SVD) but also key differences. Let's compare these two factorizations:\n",
    "\n",
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "SVD is a general factorization method for any matrix, not just probability matrices. For a given matrix $P$, SVD decomposes it into three matrices:\n",
    "\n",
    "1. $P = U_{\\text{SVD}} S V_{\\text{SVD}}^T$\n",
    "   \n",
    "   - $U_{\\text{SVD}}$ is an $M \\times M$ orthogonal matrix containing the left singular vectors of $P$.\n",
    "   - $S$ is an $M \\times N$ diagonal matrix (assuming $M \\leq N$) with non-negative diagonal entries known as singular values.\n",
    "   - $V_{\\text{SVD}}$ is an $N \\times N$ orthogonal matrix containing the right singular vectors of $P$.\n",
    "\n",
    "### The Given Factorization\n",
    "\n",
    "In the given factorization, we have:\n",
    "\n",
    "1. $P = U \\Sigma V^T$\n",
    "\n",
    "   - $U$ is an $M \\times K$ matrix representing conditional probabilities of elements in $\\mathcal{U}$ given the latent variable $z$.\n",
    "   - $\\Sigma$ is a $K \\times K$ diagonal matrix with probabilities of the latent variable $z$.\n",
    "   - $V$ is an $N \\times K$ matrix representing conditional probabilities of elements in $\\mathcal{V}$ given the latent variable $z$.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "1. **Interpretation**: In SVD, the matrices $U_{\\text{SVD}}$, $S$, and $V_{\\text{SVD}}$ are mathematical constructs without specific probabilistic interpretations. In contrast, in the given factorization, $U$, $\\Sigma$, and $V$ have direct probabilistic interpretations related to the latent variable model.\n",
    "\n",
    "2. **Dimensionality**: The dimensions of the matrices in SVD are fixed based on the dimensions of $P$. In the given factorization, the dimension $K$ (the number of latent variables) is a model choice and may not correspond to the dimensions of $P$.\n",
    "\n",
    "3. **Orthogonality**: In SVD, $U_{\\text{SVD}}$ and $V_{\\text{SVD}}$ are orthogonal matrices, which is not a requirement in the given factorization. The matrices $U$ and $V$ in the given factorization are not necessarily orthogonal.\n",
    "\n",
    "4. **Nature of Diagonal Matrix**: In SVD, the diagonal matrix $S$ contains singular values, which are not probabilities. In the given factorization, $\\Sigma$ contains probabilities of the latent variable $z$.\n",
    "\n",
    "5. **Applicability**: SVD is a general-purpose decomposition applicable to any real or complex matrix. The given factorization is specific to probabilistic models with latent variables and conditional independence assumptions.\n",
    "\n",
    "In summary, while the factor structures of SVD and the given factorization appear similar, their purposes, interpretations, and constraints are quite different. The given factorization is tailored to a probabilistic model with latent variables and interprets the matrices in the context of this model, whereas SVD is a more general matrix decomposition technique.\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2997dfcf",
   "metadata": {},
   "source": [
    "(c) In an iid sample from $\\left\\{p\\left(u_i, v_j\\right)\\right\\}$, we observe the count data $\\left\\{n_{i j}, i=1, \\ldots, M, j=1, \\ldots, N\\right\\}$, i.e., items $u_i$ and $v_j$ co-occured $n_{i j}$ items in the sample. Derive an EM algorithm that estimates the parameter matrices $U, V, \\Sigma$ based on those count data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d0e7f",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "### Problem Setup\n",
    "\n",
    "- We have count data $n_{ij}$ indicating the number of times $u_i$ and $v_j$ co-occur.\n",
    "- Our goal is to estimate matrices $U, V, \\Sigma$ from this data.\n",
    "- $U$ is an $M \\times K$ matrix, $V$ is an $N \\times K$ matrix, and $\\Sigma$ is a $K \\times K$ diagonal matrix.\n",
    "- The latent variable $z$ takes values in $\\{1, \\ldots, K\\}$.\n",
    "\n",
    "### EM Algorithm\n",
    "\n",
    "#### E-step: Expectation\n",
    "\n",
    "In the E-step, we calculate the expected value of the latent variable given the observed data and the current estimates of our parameters. Specifically, we compute the expected value of the indicator variable that $z = k$ given the observed co-occurrence of $u_i$ and $v_j$. Denote this expectation as $\\gamma_{ijk}$, which is the posterior probability that the $k$-th latent variable generated the observation $(u_i, v_j)$.\n",
    "\n",
    "$$\n",
    "\\gamma_{ijk} = \\frac{p(u_i, v_j \\mid z=k) p(z=k)}{\\sum_{l=1}^{K} p(u_i, v_j \\mid z=l) p(z=l)}\n",
    "$$\n",
    "\n",
    "Given our model,\n",
    "\n",
    "$$\n",
    "\\gamma_{ijk} = \\frac{p(u_i \\mid z=k) p(v_j \\mid z=k) p(z=k)}{\\sum_{l=1}^{K} p(u_i \\mid z=l) p(v_j \\mid z=l) p(z=l)}\n",
    "$$\n",
    "\n",
    "#### M-step: Maximization\n",
    "\n",
    "In the M-step, we update our estimates of $U, V, \\Sigma$ to maximize the expected complete-data log likelihood. The updates are done as follows:\n",
    "\n",
    "1. **Update for $U$ and $V$:** We update each element of $U$ and $V$ by maximizing the expected log likelihood. For $U$ and $V$, the updates are:\n",
    "\n",
    "   $$\n",
    "   U_{ik} \\leftarrow \\frac{\\sum_{j=1}^{N} n_{ij} \\gamma_{ijk}}{\\sum_{j=1}^{N} \\sum_{l=1}^{K} n_{ij} \\gamma_{ijl}}\n",
    "   $$\n",
    "   $$\n",
    "   V_{jk} \\leftarrow \\frac{\\sum_{i=1}^{M} n_{ij} \\gamma_{ijk}}{\\sum_{i=1}^{M} \\sum_{l=1}^{K} n_{ij} \\gamma_{ijl}}\n",
    "   $$\n",
    "\n",
    "2. **Update for $\\Sigma$:** The diagonal elements of $\\Sigma$ are updated as follows:\n",
    "\n",
    "   $$\n",
    "   \\Sigma_{kk} \\leftarrow \\frac{\\sum_{i=1}^{M} \\sum_{j=1}^{N} n_{ij} \\gamma_{ijk}}{\\sum_{i=1}^{M} \\sum_{j=1}^{N} n_{ij}}\n",
    "   $$\n",
    "\n",
    "### Iterative Process\n",
    "\n",
    "- **Initialization:** Start with initial guesses for $U, V, \\Sigma$.\n",
    "- **Iterate until convergence:**\n",
    "  - Perform the E-step, calculating $\\gamma_{ijk}$ using the current parameter estimates.\n",
    "  - Perform the M-step, updating the estimates of $U, V, \\Sigma$ using the computed $\\gamma_{ijk}$.\n",
    "- **Convergence:** The algorithm is typically iterated until the change in the log likelihood (or the change in parameter estimates) falls below a predefined threshold.\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd315af2",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "Consider a multiclass text classification problem where we have $l$ labeled documents and $u$ unlabeled ones where the label is missing $\\left(x^{(1)}, y^{(1)}\\right), \\ldots,\\left(x^{(l)}, y^{(l)}\\right), x^{(l+1)}, \\ldots, x^{(l+u)}$. All documents from class $r$ were generated from a multinomial or naive Bayes distribution with parameter $\\theta^{(r)}$.\n",
    "(a) Derive is the maximum likelihood estimator for $\\theta^{(r)}$ using only the labeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dffc4d",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "To derive the maximum likelihood estimator for $\\theta^{(r)}$ in a multiclass text classification problem using a Naive Bayes model, we will follow these steps:\n",
    "\n",
    "1. **Model Setup**: Given $l$ labeled documents $\\left(x^{(i)}, y^{(i)}\\right)$ for $i = 1, \\ldots, l$, where $x^{(i)}$ represents the features (e.g., word counts or frequencies) of the $i$-th document and $y^{(i)}$ its class label. Assume there are $K$ classes, and $r \\in \\{1, \\ldots, K\\}$ represents a specific class. Each class $r$ is characterized by a multinomial distribution with parameter vector $\\theta^{(r)} = \\left( \\theta^{(r)}_1, \\ldots, \\theta^{(r)}_V \\right)$, where $V$ is the vocabulary size, and $\\theta^{(r)}_j$ is the probability of word $j$ occurring in a document of class $r$.\n",
    "\n",
    "2. **Likelihood Function**: The likelihood of observing the data for class $r$ is the product of the probabilities of each word in the documents of class $r$, assuming independence (Naive Bayes assumption). For a single document $x^{(i)}$, the likelihood under class $r$ is:\n",
    "   $$\n",
    "   P\\left(x^{(i)} | \\theta^{(r)}\\right) = \\prod_{j=1}^{V} \\left( \\theta_j^{(r)} \\right)^{x_j^{(i)}}\n",
    "   $$\n",
    "   where $x_j^{(i)}$ is the count or frequency of word $j$ in document $i$.\n",
    "\n",
    "3. **Total Likelihood for Labeled Data**: We consider only the documents labeled as class $r$. The total likelihood for all such documents is:\n",
    "   $$\n",
    "   L(\\theta^{(r)}) = \\prod_{i: y^{(i)} = r} P\\left(x^{(i)} | \\theta^{(r)}\\right) = \\prod_{i: y^{(i)} = r} \\prod_{j=1}^{V} \\left( \\theta_j^{(r)} \\right)^{x_j^{(i)}}\n",
    "   $$\n",
    "\n",
    "4. **Log-Likelihood**: To make the computation easier, we take the logarithm of the likelihood:\n",
    "   $$\n",
    "   \\log L(\\theta^{(r)}) = \\sum_{i: y^{(i)} = r} \\sum_{j=1}^{V} x_j^{(i)} \\log \\theta_j^{(r)}\n",
    "   $$\n",
    "\n",
    "5. **Constraints**: The parameters $\\theta^{(r)}$ must satisfy two constraints:\n",
    "   - Non-negativity: $\\theta_j^{(r)} \\geq 0$ for all $j$.\n",
    "   - Normalization: $\\sum_{j=1}^{V} \\theta_j^{(r)} = 1$.\n",
    "\n",
    "6. **Maximization with Constraint**: We maximize $\\log L(\\theta^{(r)})$ subject to the normalization constraint. This is typically done using a Lagrange multiplier. Let $\\lambda$ be the Lagrange multiplier for the constraint. The Lagrangian is:\n",
    "   $$\n",
    "   \\mathcal{L}(\\theta^{(r)}, \\lambda) = \\sum_{i: y^{(i)} = r} \\sum_{j=1}^{V} x_j^{(i)} \\log \\theta_j^{(r)} - \\lambda \\left( \\sum_{j=1}^{V} \\theta_j^{(r)} - 1 \\right)\n",
    "   $$\n",
    "\n",
    "7. **Derivative and Setting to Zero**: We take the derivative of $\\mathcal{L}$ with respect to $\\theta_j^{(r)}$ and set it to zero to find the maximum:\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial \\theta_j^{(r)}} = \\sum_{i: y^{(i)} = r} \\frac{x_j^{(i)}}{\\theta_j^{(r)}} - \\lambda = 0\n",
    "   $$\n",
    "\n",
    "8. **Solving for $\\theta^{(r)}$ and $\\lambda$**:\n",
    "   - From the derivative equation, we can express $\\theta_j^{(r)}$ in terms of $\\lambda$:\n",
    "     $$\n",
    "     \\theta_j^{(r)} = \\frac{1}{\\lambda} \\sum_{i: y^{(i)} = r} x_j^{(i)}\n",
    "     $$\n",
    "   - Apply the normalization constraint $\\sum_{j=1}^{V} \\theta_j^{(r)} = 1$ to solve for $\\lambda$:\n",
    "     $$\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103410d2",
   "metadata": {},
   "source": [
    "(b) What is the mean squared error of this estimator?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91f38b0",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66179791",
   "metadata": {},
   "source": [
    "(c) We can construct an estimator for $\\theta^{(1)}, \\ldots, \\theta^{(k)}$ that uses the unlabeled as well as the labeled data by maximizing the likelihood of the observed data\n",
    "$$\n",
    "\\sum_{i=1}^l \\log p\\left(x^{(i)}, y^{(i)}\\right)+\\sum_{i=l+1}^{l+u} \\log p\\left(x^{(i)}\\right) .\n",
    "$$\n",
    "\n",
    "Simplify the loglikelihood above as much as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5f5b61",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d8faa",
   "metadata": {},
   "source": [
    "(d) Show how the EM algorithm can be used to find the MLE in (c).\n",
    "\n",
    "In your answers please use $V$ to denote the vocabulary, $k$ to denote the number of classes, and $c\\left(x^{(i)}, w\\right)$ to denote the number of times word $w$ appeard in document $x^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bef0493",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84629aa9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90d75815",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "Principal component analysis is a technique to embed high dimensional data $x^{(1)}, \\ldots, x^{(n)} \\in \\mathbb{R}^d$ in a low dimensionsal space $z^{(1)}, \\ldots, z^{(n)} \\in \\mathbb{R}^l$ with $l \\ll d$.\n",
    "(a) Write down a detailed description of the PCA algorithm. Specifically, explain how the high dimensional data are used to compute the dimensionality reduction and provide a formula for the coordinates of the reduced dimensional data $z^{(i)}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b92ca",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b065f1",
   "metadata": {},
   "source": [
    "(b) Describe a way to measure the amount of distortion caused by PCA and a principled way to determine what is an appropriate value of $l$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4128e8e",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ce312",
   "metadata": {},
   "source": [
    "(c) Assume that $x^{(1)}, \\ldots, x^{(n)} \\sim N(0, \\Sigma)$ where $\\Sigma$ is a diagonal matrix. What will be the PCA embedding in the limit of large data $n \\rightarrow \\infty$ in terms of $\\Sigma$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ab1b2",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c95d88",
   "metadata": {},
   "source": [
    "(d) Repeat (c) above for a non-diagonal matrix $\\Sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac30e3dc",
   "metadata": {},
   "source": [
    "<div style=\"color:blue\">\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6405f3d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
